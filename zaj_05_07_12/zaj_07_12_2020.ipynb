{
 "cells": [
  {
   "source": [
    "# BGT zadanie 5\n",
    "## Autors\n",
    "- Paweł Benkowski\n",
    "- Stanisław Lutkiewicz\"\n",
    "### Task\n",
    "Downloads Nietzsche and freud books count group of words in it(tiny,small,medium,big).  \n",
    "Match language by analyzing character frequency  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: PyPDF2 in /opt/conda/lib/python3.8/site-packages (1.26.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (2.25.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.54.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyPDF2\n",
    "!{sys.executable} -m pip install requests\n",
    "!{sys.executable} -m pip install bs4\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkFiles\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "import timeit\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_urls(url):\n",
    "    urlsWithName = []\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        arr = a[\"href\"].split(\".\")\n",
    "        if arr[-1] == \"pdf\":\n",
    "            urlsWithName.append((\"http:\" +a[\"href\"],a.string))\n",
    "    return urlsWithName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_and_create_folder(urls, target):\n",
    "    if not os.path.isdir(target):\n",
    "        os.makedirs(target)\n",
    "    for url in urls:\n",
    "        file = Path(target,url[1] + \".pdf\")\n",
    "        if not file.exists():\n",
    "            response = requests.get(url[0])\n",
    "            with open(Path(target, url[1] + \".pdf\"), \"wb+\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(\"File already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_from_pdf(name):\n",
    "    file = Path(name + \".txt\")\n",
    "    if not file.exists():\n",
    "        fileReader = PyPDF2.PdfFileReader(name)\n",
    "        with open(Path(name + \".txt\"), \"a+\") as f:\n",
    "            for page in fileReader.pages:\n",
    "                f.write(page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(word : str) -> bool:\n",
    "    if \"s\" == word or \"i\" == word:\n",
    "        return False\n",
    "    partOfSpeech = nltk.pos_tag([word])\n",
    "    if \"NN\" == partOfSpeech[0][1]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_text_files(textFiles):\n",
    "    print(textFiles)\n",
    "    if len(textFiles) == 1 :\n",
    "        return textFiles[0].name\n",
    "    with open(textFiles[0],'a') as toAppend:\n",
    "        for txt in textFiles[1::]:\n",
    "            with open(txt) as fromAppend:\n",
    "                toAppend.write(fromAppend.read())\n",
    "            os.remove(txt)\n",
    "    print(textFiles)    \n",
    "    return textFiles[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File already exists\n",
      "[PosixPath('freud/Sigmund Freud – The Complete Works.pdf.txt')]\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n",
      "[PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL V.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL X.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XIV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL I.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL II.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL IV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL III.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL IX.pdf.txt')]\n",
      "[PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL V.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL X.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XIV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XVIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VIII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL I.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL II.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL XII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL IV.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VI.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL III.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL VII.pdf.txt'), PosixPath('nitsche/The complete works of Friedrich Nietzsche VOL IX.pdf.txt')]\n"
     ]
    }
   ],
   "source": [
    "folderNamesWithUrls = [('freud',\"https://holybooks.com/sigmund-freud-the-complete-works/\"),\\\n",
    "    ('nitsche',\"https://holybooks.com/the-complete-works-of-friedrich-nietzsche-in-english-as-pdf/\")]\n",
    "files = []\n",
    "for fnwu in folderNamesWithUrls:\n",
    "    urls = get_books_urls(fnwu[1])\n",
    "    get_books_and_create_folder(urls, fnwu[0])\n",
    "    pdfFiles = list(Path(fnwu[0]).glob('*.pdf'))\n",
    "    for pdf in pdfFiles:\n",
    "        save_text_from_pdf(os.path.join(fnwu[0], pdf.name))\n",
    "    textFileName = compose_text_files(list(Path(fnwu[0]).glob('*.txt')))\n",
    "    files.append(sc.textFile(os.path.join(fnwu[0], textFileName)).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_letters_and_print_ratio_and_language(lettersRdd):\n",
    "    lettersForCounting = lowerCaseLetters.map(lambda x: (x,1))\n",
    "    countedLetters =  lettersForCounting.reduceByKey(add).collect()\n",
    "    kLetters = Counter(dict(countedLetters))\n",
    "    mostCommonLetters = kLetters.most_common(3)\n",
    "    allLetters = lowerCaseLetters.count()\n",
    "    print(f\"top letters {mostCommonLetters}\")\n",
    "    print(f\"total letters {allLetters}\")\n",
    "    mostCommonLettersRatio = {}\n",
    "    for mcl in mostCommonLetters:\n",
    "        mostCommonLettersRatio[mcl[0]] = (mcl[1]/allLetters)*100\n",
    "        print(f\"{mcl[0]} : {mostCommonLettersRatio[mcl[0]]}%\")\n",
    "    print(match_language(mostCommonLettersRatio))\n",
    "\n",
    "\n",
    "def split_ratio(ratioString):\n",
    "    ratioDict = {}\n",
    "    for l in ratioString.split(\",\"):\n",
    "        ls = l.split(\":\")\n",
    "        ratioDict[ls[0].lower()]=float(ls[1])\n",
    "    return ratioDict\n",
    "\n",
    "\n",
    "def match_language(topLetters):\n",
    "    englishRatio = \"A:8.55,B:1.60,C:3.16,D:3.87,E:12.10,F:2.18,G:2.09,H:4.96,I:7.33,J:0.22,K:0.81,L:4.21,M:2.53,N:7.17,O:7.47,P:2.07,Q:0.10,R:6.33,S:6.73,T:8.94,U:2.68,V:1.06,W:1.83,X:0.19,Y:1.72,Z:0.11\"\n",
    "    polishRatio = \"a:8.91,i:8.21,o:7.75,e:7.66,z:5.64,n:5.52,r:4.69,w:4.65,s:4.32,t:3.98,c:3.96,y:3.76,k:3.51,d:3.25,p:3.13,m:2.80,u:2.50,j:2.28,l:2.10,ł:1.82,b:1.47,g:1.42,ę:1.11,h:1.08,ą:0.99,ó:0.85,ż:0.83,ś:0.66,ć:0.40,f:0.30,ń:0.20,q:0.14,ź:0.06,v:0.04,x:0.02\"\n",
    "    englishRatioDict = split_ratio(englishRatio)\n",
    "    polishRatioDict = split_ratio(polishRatio)\n",
    "    winners = {}\n",
    "    for key,value in topLetters.items():\n",
    "        if(value-englishRatioDict[key] > value-polishRatioDict[key]):\n",
    "            winners[key] = \"polish\"\n",
    "        else:\n",
    "            winners[key] = \"english\"\n",
    "    polishWinCounter=0\n",
    "    for key,value in winners.items():\n",
    "        if value == \"polish\":\n",
    "            polishWinCounter+=1\n",
    "    if(polishWinCounter>2):\n",
    "        return \"polish\"\n",
    "    else:\n",
    "        return \"english\"\n",
    "\n",
    "def create_groups_and_print_count(words):\n",
    "    kTiny = words.filter(lambda x: len(x) < 2 ).count()\n",
    "    kSmall = words.filter(lambda x: 1 < len(x) < 5 ).count()\n",
    "    kMedium = words.filter(lambda x: 4 < len(x) < 10 ).count()\n",
    "    kBig = words.filter(lambda x: len(x) > 9 ).count()\n",
    "    print(f\"tiny {kTiny}\")\n",
    "    print(f\"small {kSmall}\")\n",
    "    print(f\"medium {kMedium}\")\n",
    "    print(f\"big {kBig}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method RDD.name of freud/Sigmund Freud – The Complete Works.pdf.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0>\n",
      "top letters [('e', 1204065), ('t', 931463), ('i', 759718)]\n",
      "total letters 9572627\n",
      "e : 12.578208677722428%\n",
      "t : 9.730484641258872%\n",
      "i : 7.936358535645439%\n",
      "english\n",
      "tiny 81428\n",
      "small 1152818\n",
      "medium 693893\n",
      "big 140767\n",
      "total word 2068906\n",
      "most common [('the', 142371), ('of', 94924), ('to', 60988), ('in', 50950), ('a', 48808), ('and', 46208), ('that', 33805), ('is', 30431), ('it', 27788), ('which', 22649)]\n",
      "top nouns [('dream', 6115), ('analysis', 3584), ('time', 3456)]\n",
      "time spend 54.0358791059698\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "<bound method RDD.name of nitsche/The complete works of Friedrich Nietzsche VOL XVI.pdf.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0>\n",
      "top letters [('e', 3632535), ('t', 2790736), ('i', 2285251)]\n",
      "total letters 29295089\n",
      "e : 12.399808718792423%\n",
      "t : 9.52629295647472%\n",
      "i : 7.800798966680047%\n",
      "english\n",
      "tiny 253108\n",
      "small 3669810\n",
      "medium 2190070\n",
      "big 359356\n",
      "total word 6472344\n",
      "most common [('the', 417236), ('of', 296467), ('and', 218803), ('to', 182764), ('a', 129587), ('in', 123494), ('is', 105357), ('that', 83336), ('it', 71997), ('as', 66403)]\n",
      "top nouns [('man', 21340), ('life', 13390), ('power', 12636)]\n",
      "time spend 134.1375765010016\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "total time spend 188.17446956201456\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "# pfiles = sc.parallelize(files).foreach(do_work)\n",
    "for file in files: # paralize -> foreach\n",
    "    startIn = timeit.default_timer()\n",
    "    print(file.name)\n",
    "    lowerCaseText = file.map(lambda x: x.lower())\n",
    "    lowerCaseLetters = lowerCaseText.flatMap(lambda x: re.findall(r\"[a-zA-Z]\", x))\n",
    "    count_letters_and_print_ratio_and_language(lowerCaseLetters)\n",
    "\n",
    "    words = lowerCaseText.flatMap(lambda x : re.findall(r\"\\b[^\\d\\W]+\\b\", x))\n",
    "    create_groups_and_print_count(words)\n",
    "\n",
    "    wordsForCounting = words.map(lambda x: (x,1))\n",
    "    countedNouns = wordsForCounting.reduceByKey(add).filter(lambda x: is_noun(x[0])).collect()\n",
    "\n",
    "    kNouns = Counter(dict(countedNouns))\n",
    "    kWords = Counter(words.collect())\n",
    "    \n",
    "    print(f\"total word {words.count()}\")\n",
    "    print(f\"most common {kWords.most_common(10)}\")\n",
    "    print(f\"top nouns {kNouns.most_common(3)}\")\n",
    "    print(f\"time spend {timeit.default_timer() - startIn}\")\n",
    "    print(30*\"----\")\n",
    "print(f\"total time spend {timeit.default_timer()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "covid_19_india.csv  freud  nitsche  nltk_data  work\n",
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "sc.addFile(\"hdfs://namenode:9000/covid_19_india.csv\")\n",
    "\n",
    "\n",
    "# 'hdfs:///user/bekce/myfile.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: hadoop: command not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}